# ðŸš™ Automama â€“ AGV Software Stack

This repository contains the software stack for **Project Automama (AGV)**.  
It integrates custom perception, stereo vision, and autonomous navigation pipelines with low-level actuation through an **ESP32 microcontroller**.

---

## ðŸ§© System Overview

- **Camera Perception** â€“ Stereo image capture, depth estimation, and segmentation for obstacle detection.
- **Stereo Vision Pipeline** â€“ Custom disparity â†’ depth â†’ 3D point cloud reconstruction, accelerated with GPU and visualized in real time with VisPy.
- **Navigation Stack (ROS2)** â€“ Localization, costmaps, and path planning modules for autonomous driving.
- **Control & Actuation** â€“ ROS2 communicates with **ESP32** to control:
  - Steering  
  - Throttle  
  - Braking  
- **Manual Override** â€“ Python-Arduino bridge for teleoperation and debugging.

---

## ðŸ“‚ Project Automama â€“ Directory Structure

```text
automama
â”œâ”€â”€ arduino codes/                # Arduino control scripts
â”‚   â”œâ”€â”€ Automama_control_test_DualCore/
â”‚   â”‚   â””â”€â”€ *.ino                 # Thruster, steering, brake, comms control
â”‚   â””â”€â”€ py_control.py             # Python-side Arduino control interface
â”‚
â”œâ”€â”€ automama/
â”‚   â”œâ”€â”€ callab_data/              # Camera calibration files (stereo + intrinsic)
â”‚   â”œâ”€â”€ control/                  # Manual + autonomous control scripts
â”‚   â”œâ”€â”€ interface/                # Data logging and interface scripts
â”‚   â”œâ”€â”€ navigation/               # Costmaps, GPS tests, VO modules
â”‚   â”œâ”€â”€ perception/               # Stereo vision, depth estimation, segmentation
â”‚   â””â”€â”€ test/                     # Unit tests for communication & control
â”‚
â”œâ”€â”€ daddyutils/                   # Utility modules (camera handling, GPU, SLAM utils)
â”œâ”€â”€ launch/                       # ROS2 launch files for stereo pipeline & nodes
â”œâ”€â”€ resource/                     # ROS2 resource files
â”œâ”€â”€ test/                         # Code quality & style tests
â”‚
â”œâ”€â”€ output_video.mp4              # Sample output (stereo pipeline demo)
â”œâ”€â”€ stereo.rviz                   # RViz config for visualization
â”œâ”€â”€ sust_campus_map.html          # Campus map visualization
â”‚
â”œâ”€â”€ setup.py                      # Package setup
â”œâ”€â”€ setup.cfg
â”œâ”€â”€ pyproject.toml
â””â”€â”€ package.xml                   # ROS2 package manifest
```

---
## âš™ï¸ Technology Stack

- **ROS2 (Foxy/Humble)** â€“ Middleware for modular control and communication  
- **Python + OpenCV** â€“ Image processing & stereo disparity  
- **VisPy (GPU)** â€“ Real-time 3D point cloud visualization  
- **ESP32 + Arduino** â€“ Low-level actuation (steering, brake, throttle)  
- **RViz2** â€“ Simulation & visualization  

---

## ðŸš€ Features

- Custom stereo vision â†’ depth â†’ 3D point cloud pipeline  
- Autonomous navigation with ROS2 planners & costmaps  
- Real-time actuation via ESP32 bridge  
- Simulation + real-world testing support  
- Modular design for future extensions  

---
## ðŸ§  Perception & Navigation Pipeline

In this module, we designed a **GPU-accelerated stereo vision + semantic perception stack** optimized for real-time operation on the Jetson Orin Nano.

### Workflow

1. **Stereo Camera Input** â†’ Capture and rectify left/right frames.  
2. **Semantic Segmentation** â†’ Apply YOLOv8n-Seg on the left frame to extract class-wise segmentation masks.  
3. **Depth Estimation**  
   - **Road Mask** â†’ Use static camera projection for depth estimation.  
   - **Dynamic Objects** â†’ Apply custom stereo disparity + 3D reconstruction for object depth.  
4. **Occupancy Grid Mapping** â†’ Fuse road and object depth into a unified grid map.  
5. **Path Planning** â†’ Run **Gap Follow Algorithm** for real-time dynamic path planning.  

### Optimization

- Entire pipeline runs on **GPU** for real-time performance.  
- Integrated with **NVIDIA VPI**, **CUDA**, and **CuPy** to handle:  
  - GPU memory context management  
  - Custom CUDA kernel insertions  
  - Concurrent processing of segmentation + stereo vision  
- Achieved **real-time inference + planning** on **Jetson Orin Nano**.


```mermaid
graph TD
    A[Stereo Camera Input] --> B[Rectification]
    B --> C[Left Frame: YOLOv8n-Seg]
    C --> D[Segmentation Masks]
    D --> E{Mask Type?}
    E -->|Road| F[Static Camera Projection â†’ Depth]
    E -->|Dynamic Objects| G[Stereo Vision Pipeline â†’ Object Depth]
    F --> H[Occupancy Grid Mapping]
    G --> H
    H --> I[Gap Follow Algorithm â†’ Real-Time Path Planning]
    style A fill:#d9f0ff,stroke:#333,stroke-width:1px
    style B fill:#ffe6cc,stroke:#333,stroke-width:1px
    style C fill:#e6ccff,stroke:#333,stroke-width:1px
    style D fill:#fff0b3,stroke:#333,stroke-width:1px
    style F fill:#ccffcc,stroke:#333,stroke-width:1px
    style G fill:#ffcccc,stroke:#333,stroke-width:1px
    style H fill:#ffd9e6,stroke:#333,stroke-width:1px
    style I fill:#cce6ff,stroke:#333,stroke-width:1px
```
